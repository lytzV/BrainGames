{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brain Games 3-classification using Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import sklearn\n",
    "import scipy.cluster\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy import signal\n",
    "import time\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30577, 6)\n",
      "There are 51 datapoints per window, and there are 599 windows.\n"
     ]
    }
   ],
   "source": [
    "csvData = []\n",
    "csvData.append(np.genfromtxt('DoingnothingbutnotblankingoutJustthinkspontaneouslyNoparticularTriggerEvent.csv', delimiter=','))\n",
    "csvData.append(np.genfromtxt('2min1-100additionsubtraction.csv', delimiter=','))\n",
    "csvData.append(np.genfromtxt('2min1-50multiplication.csv', delimiter=','))\n",
    "print(csvData[0].shape)\n",
    "sample_time = 120\n",
    "sample_len = csvData[0].shape[0]\n",
    "window_len = 0.2 #how long is each window in terms of seconds\n",
    "window_num = (int)(sample_time//window_len) \n",
    "data_per_window = (int)(sample_len//(window_num)) #data_per_window = datapoints per window = sample_len // (window_num)\n",
    "#assert(data_per_window % 2 == 0)\n",
    "\n",
    "print(\"There are\",data_per_window, \"datapoints per window, and there are\", window_num, \"windows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 30576, 5)\n"
     ]
    }
   ],
   "source": [
    "channel = []\n",
    "for d in csvData:\n",
    "    channel.append(d[1:,1:])\n",
    "    #print(d.shape)\n",
    "    #plt.plot(rawData[:,:])\n",
    "    #plt.xlim((0,rawData.shape[0]))\n",
    "    #plt.title('A Very Representative Feature VS Full Time')\n",
    "    #plt.figure(figsize = (16,9))\n",
    "print(np.asarray(channel).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 28 points left out by segmentation, now cutting them from both ends...\n",
      "(3, 30549, 5)\n"
     ]
    }
   ],
   "source": [
    "residual = sample_len % data_per_window\n",
    "print(\"There are\", residual, \"points left out by segmentation, now cutting them from both ends...\")\n",
    "residualStart = residual // 2\n",
    "residualEnd = residual - residualStart\n",
    "\n",
    "pruned = []\n",
    "for c in channel:\n",
    "    pruned.append(c[residualStart : -residualEnd+1,:])\n",
    "pruned = np.asarray(pruned)\n",
    "print(pruned.shape)\n",
    "for p in pruned:\n",
    "    assert np.asarray(p).shape[0] % data_per_window == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stft(nfft, data):\n",
    "    #nfft should be the closest power of 2 to data_per_window\n",
    "    f,t,Zxx = scipy.signal.stft(data, nperseg=data_per_window, return_onesided=False, padded=True, nfft=nfft)\n",
    "    #assert (Zxx.shape[1] == 2*window_num*len(csvData) + 1)\n",
    "    #print(\"There are\", Zxx.shape[1],\"time window captured by STFT...\")\n",
    "    Zxx = Zxx.T #Zxx = new data matrix --> time windows are our new data points and frequency are the features\n",
    "    #We sacrificed segements of data to expand feature size from 1 to 256\n",
    "    #print(Zxx.shape)\n",
    "    #print(\"Parsing each DFT to extract the actual coefficients used in the trucated Fourier Series...\")\n",
    "    for i in range(0,Zxx.shape[0]):\n",
    "        chnl = Zxx[i]\n",
    "        chnl[0] = chnl[0].real\n",
    "        chnl[1:nfft//2] = 1*chnl[1:nfft//2].real\n",
    "        chnl[nfft//2] = chnl[nfft//2]\n",
    "        chnl[nfft//2 + 1:] = (1)*chnl[nfft//2 + 1:].imag\n",
    "    assert ((Zxx.real == Zxx).all())\n",
    "    #print(\"Coefficient extracted!\")\n",
    "    Zxx = Zxx.real #The imaginary compoenent is 0 by now, we are just getting rid of the j for PCA\n",
    "    #Since PCA from sklearn doesn't handle complex data, let us simply use the weight of thecosine part of the frequency space\n",
    "    return Zxx\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing each DFT to extract the actual coefficients used in the trucated Fourier Series...\n",
      "Coefficient extracted!\n",
      "Parsing each DFT to extract the actual coefficients used in the trucated Fourier Series...\n",
      "Coefficient extracted!\n",
      "Parsing each DFT to extract the actual coefficients used in the trucated Fourier Series...\n",
      "Coefficient extracted!\n",
      "(3, 1176, 320)\n"
     ]
    }
   ],
   "source": [
    "nfft = 64 #depending on the window size\n",
    "#We again reduce the data size and increase the features from 5 to 5*nfft\n",
    "post_stft_dataset = []\n",
    "for p in pruned:\n",
    "    expanded_features_dataset = []\n",
    "    print(\"Parsing each DFT to extract the actual coefficients used in the trucated Fourier Series...\")\n",
    "    for i in range(p.shape[1]):\n",
    "        channel = p[:,i]\n",
    "        stft_results = stft(nfft, channel)\n",
    "        expanded_features_dataset.append(stft_results)\n",
    "    expanded_features_dataset = np.asarray(expanded_features_dataset)\n",
    "    hstacked = expanded_features_dataset[0]\n",
    "    for expanded_channel in expanded_features_dataset[1:]:\n",
    "        hstacked = np.hstack((hstacked, expanded_channel))\n",
    "    post_stft_dataset.append(hstacked)\n",
    "    print(\"Coefficient extracted!\")\n",
    "post_stft_dataset = np.asarray(post_stft_dataset)\n",
    "print(post_stft_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(dataset, label, ratio):\n",
    "    split = (int)(dataset.shape[0] * ratio)\n",
    "    y = label * np.ones(dataset.shape[0])\n",
    "    return dataset[:split], dataset[split:], y[:split], y[split:]\n",
    "def conglomerates(three_d_matrix):\n",
    "    vstacked = three_d_matrix[0]\n",
    "    for m in three_d_matrix[1:]:\n",
    "        vstacked = np.vstack((vstacked, m))\n",
    "    return vstacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = [],[],[],[]\n",
    "for i in range(post_stft_dataset.shape[0]):\n",
    "    xtr, xt, ytr, yt = train_test_split(post_stft_dataset[i],i,0.8)\n",
    "    X_train.append(xtr)\n",
    "    X_test.append(xt)\n",
    "    y_train.append(ytr)\n",
    "    y_test.append(yt)\n",
    "X_train = conglomerates(np.asarray(X_train))\n",
    "X_test = conglomerates(np.asarray(X_test))\n",
    "y_train = np.asarray(y_train).flatten()\n",
    "y_test = np.asarray(y_test).flatten()\n",
    "\n",
    "assert X_train.shape[0] == y_train.shape[0]\n",
    "assert X_test.shape[0] == y_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(200,20), max_iter=50, alpha=1e-5,\n",
    "                    solver='adam', verbose=10, tol=1e-4, random_state=1,\n",
    "                    learning_rate_init=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = inf\n",
      "Iteration 2, loss = 1.10595624\n",
      "Iteration 3, loss = 1.10398726\n",
      "Iteration 4, loss = 1.10061907\n",
      "Iteration 5, loss = 1.10261097\n",
      "Iteration 6, loss = 1.10428943\n",
      "Iteration 7, loss = 1.10435145\n",
      "Iteration 8, loss = 1.09970913\n",
      "Iteration 9, loss = 1.09998477\n",
      "Iteration 10, loss = 1.10071772\n",
      "Iteration 11, loss = 1.10251488\n",
      "Iteration 12, loss = 1.10019768\n",
      "Iteration 13, loss = 1.10207406\n",
      "Iteration 14, loss = 1.10099891\n",
      "Iteration 15, loss = 1.10466568\n",
      "Iteration 16, loss = 1.09978070\n",
      "Iteration 17, loss = 1.10142957\n",
      "Iteration 18, loss = 1.10180977\n",
      "Iteration 19, loss = 1.10058691\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training set score: 0.333333\n",
      "Test set score: 0.333333\n"
     ]
    }
   ],
   "source": [
    "mlp.fit(X_train, y_train)\n",
    "print(\"Training set score: %f\" % mlp.score(X_train, y_train))\n",
    "print(\"Test set score: %f\" % mlp.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
