{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brain Games 3-classification using Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check pyriemman https://github.com/alexandrebarachant\n",
    "#definitely break down to waves and train them (P300, Alpha...)-->get relative power\n",
    "#Plot power spetrum to see if there is noise in the higher frequencies\n",
    "#Focus != Difficulty\n",
    "#Detect Relaxation (grow flowers, encourage relaxation)\n",
    "#Real time stream automatic data preprocessing problem\n",
    "#K-mean clustering fuzzy coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import sklearn\n",
    "import scipy.cluster\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy import signal\n",
    "import time\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.utils import shuffle\n",
    "import math\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import mne\n",
    "from mne.decoding import Vectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.model_selection import cross_val_score, StratifiedShuffleSplit\n",
    "#Try directly installing pyriemann in jupyter notebook if conda/pip doesn't make it\n",
    "from pyriemann.estimation import ERPCovariances\n",
    "from pyriemann.tangentspace import TangentSpace\n",
    "from pyriemann.classification import MDM\n",
    "from pyriemann.spatialfilters import Xdawn\n",
    "from collections import OrderedDict\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(sample_time, window_time, verbose, filelist):\n",
    "    csvData = []\n",
    "    for a in filelist:\n",
    "        csvData.append(np.genfromtxt(a, delimiter=','))\n",
    "    min_len = min([c.shape[0] for c in csvData])\n",
    "    if verbose: print(\"There are {l} raw data points\".format(l=min_len))\n",
    "    csvData = [c[:min_len,:] for c in csvData]\n",
    "    sample_len = csvData[0].shape[0] - 1\n",
    "    window_num = (int)(sample_time//window_time) \n",
    "    data_per_window = (int)(sample_len//(window_num)) #data_per_window = datapoints per window = sample_len // (window_num)\n",
    "    #assert(data_per_window % 2 == 0)\n",
    "\n",
    "    if verbose: print(\"There are\",data_per_window, \"datapoints per window, and there are\", window_num, \"independent windows.\")\n",
    "    channel = []\n",
    "    for d in csvData:\n",
    "        channel.append(d[1:,1:])\n",
    "        \"\"\"print(d.shape)\n",
    "        plt.plot(d[2000:-1000,2:3])\n",
    "        plt.xlim((0,d.shape[0]))\n",
    "        plt.title('A Very Representative Feature VS Full Time')\n",
    "        plt.figure(figsize = (16,9))\"\"\"\n",
    "    #print(np.asarray(channel).shape)\n",
    "    #trimData(sample_len, data_per_window, channel)\n",
    "    return sample_len, data_per_window, channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trimData(sample_len, data_per_window, channel, verbose):\n",
    "    residual = sample_len % data_per_window\n",
    "    if verbose: print(\"There are\", residual, \"points left out by segmentation, now cutting them from both ends...\")\n",
    "    residualStart = residual // 2\n",
    "    residualEnd = residual - residualStart\n",
    "    residualEnd = len(channel[0]) - residualEnd\n",
    "    #print(len(channel[0]), residualStart, residualEnd)\n",
    "\n",
    "    pruned = []\n",
    "    for c in channel:\n",
    "        pruned.append(c[residualStart : residualEnd,:])\n",
    "    pruned = np.asarray(pruned)\n",
    "    if verbose: print(\"The shape of the pruned dataset is: \", pruned.shape)\n",
    "    if verbose: print(\"The first number stands for the number of classes, the second number stands for raw data points and the third number stands for the number of channels\")\n",
    "    for p in pruned:\n",
    "        assert np.asarray(p).shape[0] % data_per_window == 0, np.asarray(p).shape[0] % data_per_window\n",
    "    return pruned\n",
    "def compute_band_powers(eegdata, fs, start, ratio, gap):\n",
    "    spectralized = []\n",
    "    orig_start = start\n",
    "    for i in range(eegdata.shape[0]):\n",
    "        # 1. Compute the PSD\n",
    "        winSampleLength, nbCh = eegdata.shape[1], eegdata.shape[2]\n",
    "\n",
    "        # Apply Hamming window\n",
    "        w = np.hamming(winSampleLength)\n",
    "        dataWinCentered = eegdata[i] - np.mean(eegdata[i], axis=0)  # Remove offset\n",
    "        dataWinCenteredHam = (dataWinCentered.T * w).T\n",
    "        #print(dataWinCenteredHam.shape)\n",
    "        NFFT = (int)(find_fft_len(winSampleLength)[0])\n",
    "        Y = np.fft.fft(dataWinCenteredHam, n=NFFT, axis=0) / winSampleLength\n",
    "        PSD = 2 * np.abs(Y[0:int(NFFT / 2), :])\n",
    "        f = fs / 2 * np.linspace(0, 1, int(NFFT / 2))\n",
    "        end = (int)(f[-1]*ratio)\n",
    "        assert start + gap <= end, start + gap\n",
    "        #plt.plot(f, PSD)\n",
    "        #plt.title(\"Frequencies Bins\")\n",
    "        #plt.show() #You should decide ratio by cutting off before surge\n",
    "        \n",
    "        # SPECTRAL FEATURES\n",
    "        # Average of band powers\n",
    "        \"\"\"\n",
    "        # Delta <4\n",
    "        ind_delta, = np.where(f < 4)\n",
    "        meanDelta = np.mean(PSD[ind_delta, :], axis=0) #if np.all(ind_delta > 0) else np.array(PSD.shape[1]*[1])\n",
    "        # Theta 4-8\n",
    "        ind_theta, = np.where((f >= 4) & (f <= 8))\n",
    "        meanTheta = np.mean(PSD[ind_theta, :], axis=0) #if np.all(ind_theta > 0) else np.array(PSD.shape[1]*[1])\n",
    "        # Alpha 8-12\n",
    "        ind_alpha, = np.where((f >= 8) & (f <= 12))\n",
    "        meanAlpha = np.mean(PSD[ind_alpha, :], axis=0) #if np.all(ind_alpha > 0) else np.array(PSD.shape[1]*[1])\n",
    "        # Beta 12-30\n",
    "        ind_beta, = np.where((f >= 12) & (f < 30))\n",
    "        meanBeta = np.mean(PSD[ind_beta, :], axis=0) #if np.all(ind_beta > 0) else np.array(PSD.shape[1]*[1])\n",
    "        \n",
    "        feature_vector = np.concatenate((meanDelta, meanTheta, meanAlpha,\n",
    "                                         meanBeta), axis=0)\n",
    "\n",
    "        \"\"\"\n",
    "        next_bin = start + gap\n",
    "        feature_vector = []\n",
    "        while next_bin <= end: #Here we have customary frequency bins\n",
    "            ind, = np.where((f >= start) & (f <= next_bin))\n",
    "            #We average the weights in all the interested frequency bins instead of using the full FFT weights\n",
    "            mean_of_bin = np.mean(PSD[ind, :], axis=0) #if not np.any(ind) else np.array(PSD.shape[1]*[0])\n",
    "            assert np.any(np.isnan(mean_of_bin)) == False\n",
    "            feature_vector.extend(mean_of_bin)\n",
    "            start += gap\n",
    "            next_bin += gap\n",
    "        feature_vector = np.asarray(feature_vector)\n",
    "        #feature_vector = np.log10(feature_vector)\n",
    "        spectralized.append(feature_vector)\n",
    "        assert np.any(np.isnan(feature_vector)) == False\n",
    "        start = orig_start\n",
    "    spectralized = np.array(spectralized)\n",
    "    #print(spectralized.shape)\n",
    "    return spectralized\n",
    "def stcbp(pruned, fs, data_per_window, start, ratio, gap):\n",
    "    #print(pruned.shape)\n",
    "    spectralizedAll = []\n",
    "    for i in range(pruned.shape[1]//data_per_window):\n",
    "        eeg_segment = pruned[:,i*data_per_window:(i+1)*data_per_window,:]\n",
    "        spectralizedAll.append(compute_band_powers(eeg_segment, fs, start, ratio, gap))\n",
    "    spectralizedAll = np.array(spectralizedAll)\n",
    "    spectralizedAll = np.reshape(spectralizedAll, newshape=(spectralizedAll.shape[1],spectralizedAll.shape[0],spectralizedAll.shape[2]))\n",
    "    #print(spectralizedAll.shape)\n",
    "    #You should see that the second paramter = total sample time / window length\n",
    "    return spectralizedAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_fft_len(data_per_window):\n",
    "    result = 2**math.ceil(math.log2(data_per_window))\n",
    "    return result, data_per_window/result\n",
    "\n",
    "def stft_for_one_channel(nfft, data_per_window, data):\n",
    "    #nfft should be the closest power of 2 to data_per_window\n",
    "    f,t,Zxx = scipy.signal.stft(data, nperseg=data_per_window, return_onesided=False, padded=True, nfft=nfft)\n",
    "    #assert (Zxx.shape[1] == 2*window_num*len(csvData) + 1)\n",
    "    #print(\"There are\", Zxx.shape[1],\"time window captured by STFT...\")\n",
    "    Zxx = Zxx.T #Zxx = new data matrix --> time windows are our new data points and frequency are the features\n",
    "    #We sacrificed segements of data to expand feature size from 1 to 256\n",
    "    #print(Zxx.shape)\n",
    "    #print(\"Parsing each DFT to extract the actual coefficients used in the trucated Fourier Series...\")\n",
    "    for i in range(0,Zxx.shape[0]):\n",
    "        chnl = Zxx[i]\n",
    "        chnl[0] = chnl[0].real\n",
    "        chnl[1:nfft//2] = 1*chnl[1:nfft//2].real\n",
    "        chnl[nfft//2] = chnl[nfft//2]\n",
    "        chnl[nfft//2 + 1:] = (1)*chnl[nfft//2 + 1:].imag\n",
    "    assert ((Zxx.real == Zxx).all())\n",
    "    #print(\"Coefficient extracted!\")\n",
    "    Zxx = Zxx.real #The imaginary compoenent is 0 by now, we are just getting rid of the j for PCA\n",
    "    #Since PCA from sklearn doesn't handle complex data, let us simply use the weight of thecosine part of the frequency space\n",
    "    return Zxx\n",
    "\n",
    "\n",
    "def stft(data_per_window, pruned, verbose):\n",
    "    nfft, percentage = find_fft_len(data_per_window) #depending on the window size\n",
    "    if verbose: print(\"FFT length: \", nfft)\n",
    "    #We again reduce the data size and increase the features from 5 to 5*nfft\n",
    "    post_stft_dataset = []\n",
    "    if verbose: print(\"Parsing each DFT to extract the actual coefficients used in the trucated Fourier Series...\")\n",
    "    for p in pruned:\n",
    "        expanded_features_dataset = []\n",
    "        for i in range(p.shape[1]):\n",
    "            channel = p[:,i]\n",
    "            stft_results = stft_for_one_channel(nfft, data_per_window, channel)\n",
    "            expanded_features_dataset.append(stft_results)\n",
    "        expanded_features_dataset = np.asarray(expanded_features_dataset)\n",
    "        #We will hstack the the stfted results for each channel for this particular class\n",
    "        hstacked = expanded_features_dataset[0]\n",
    "        for expanded_channel in expanded_features_dataset[1:]:\n",
    "            hstacked = np.hstack((hstacked, expanded_channel))\n",
    "        post_stft_dataset.append(hstacked)\n",
    "    if verbose: print(\"Coefficient extracted!\")\n",
    "    post_stft_dataset = np.asarray(post_stft_dataset)\n",
    "    if verbose: print(\"The transformed dataset after STFT: \", post_stft_dataset.shape)\n",
    "    if verbose: print(\"The first number stands for the number of classes, the second number stands for new data points and the third number stands for the number of features\")\n",
    "    return post_stft_dataset, nfft, percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def three_d_train_test_split(three_d_dataset, label, ratio):\n",
    "    #Outputs 3d matrices for train and test set\n",
    "    split = (int)(three_d_dataset.shape[0] * ratio)\n",
    "    y = label * np.ones(three_d_dataset.shape[0])\n",
    "    return three_d_dataset[:split], three_d_dataset[split:], y[:split], y[split:]\n",
    "\n",
    "def conglomerates(three_d_matrix):\n",
    "    #Outputs 2d matrices, stacked along the first axis\n",
    "    vstacked = three_d_matrix[0]\n",
    "    for m in three_d_matrix[1:]:\n",
    "        vstacked = np.vstack((vstacked, m))\n",
    "    return vstacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_test(post_stft_dataset, ratio):\n",
    "    X_train, X_test, y_train, y_test = [],[],[],[]\n",
    "    for i in range(post_stft_dataset.shape[0]):\n",
    "        xtr, xt, ytr, yt = three_d_train_test_split(post_stft_dataset[i],i,0.8)\n",
    "        X_train.append(xtr)\n",
    "        X_test.append(xt)\n",
    "        y_train.append(ytr)\n",
    "        y_test.append(yt)\n",
    "    X_train = conglomerates(np.asarray(X_train))\n",
    "    X_test = conglomerates(np.asarray(X_test))\n",
    "    y_train = np.asarray(y_train).flatten()\n",
    "    y_test = np.asarray(y_test).flatten()\n",
    "    \n",
    "    assert X_train.shape[0] == y_train.shape[0]\n",
    "    assert X_test.shape[0] == y_test.shape[0]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "def chunky_xy(verbose, window_len, sample_len, ratio, *args):\n",
    "    if verbose: print(\"Loading Data...\")\n",
    "    sample_len, data_per_window, channel = loadData(sample_len, window_len, verbose, args)\n",
    "    if verbose: print(\"===================================================================================\")\n",
    "    if verbose: print(\"\")\n",
    "    if verbose: print(\"Triming Data...\")\n",
    "    pruned = trimData(sample_len, data_per_window, channel, verbose)\n",
    "    if verbose: print(\"===================================================================================\")\n",
    "    if verbose: print(\"\")\n",
    "    if verbose: print(\"Spectralizing Data...\")\n",
    "    spectralized = stcbp(pruned, 256, data_per_window, 0, ratio, 5)\n",
    "    #print(spectralized)\n",
    "    if verbose: print(\"===================================================================================\")\n",
    "    if verbose: print(\"\")\n",
    "    if verbose: print(\"Training Data...\")\n",
    "    #print((data_per_window/nfft)*100,\"%\")\n",
    "    X_train, y_train, X_test, y_test = make_train_test(spectralized, 0.8)\n",
    "    X = np.vstack((X_train,X_test))\n",
    "    X = np.reshape(X, newshape=(X.shape[0],1,X.shape[1]))\n",
    "    y = np.vstack((np.reshape(y_train,newshape=(y_train.shape[0],1)),np.reshape(np.asarray(y_test), newshape=(y_test.shape[0],1))))\n",
    "    y = y.flatten() #crucial, sklearn prefers a vector not an array for labels\n",
    "    assert X.shape[0] == y.shape[0]\n",
    "    X, y = shuffle(X, y, random_state = 0)\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(max_iter, alpha, solver, X_train, y_train, X_test, y_test):\n",
    "    #adam, lbfgs, sgd\n",
    "    #alhpa: 1e-8\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(200,20), max_iter=max_iter, alpha=alpha,\n",
    "                    solver=solver, verbose=10, tol=1e-4, random_state=1,\n",
    "                    learning_rate_init=.1)\n",
    "    mlp.fit(X_train, y_train)\n",
    "    train_scr = mlp.score(X_train, y_train)\n",
    "    test_scr = mlp.score(X_test, y_test)\n",
    "    print(\"Training set score: %f\" % train_scr)\n",
    "    print(\"Test set score: %f\" % test_scr)\n",
    "    return train_scr, test_scr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window size: 1 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/victorli/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/victorli/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average precision is: [0.4777777777777778, 0.39444444444444443, 0.39444444444444443, 0.37777777777777777, 0.5305555555555556]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def run(window_len, verbose):\n",
    "    print(\"Window size:\",window_len,\"s\")\n",
    "    if verbose: print(\"Loading Data...\")\n",
    "    sample_len, data_per_window, channel = loadData(60, window_len, verbose, \"60sactualrest.csv\", \"60smultiplication5.4lvl.csv\")\n",
    "    if verbose: print(\"===================================================================================\")\n",
    "    if verbose: print(\"\")\n",
    "    if verbose: print(\"Triming Data...\")\n",
    "    pruned = trimData(sample_len, data_per_window, channel, verbose)\n",
    "    if verbose: print(\"===================================================================================\")\n",
    "    if verbose: print(\"\")\n",
    "    if verbose: print(\"STFTing Data...\")\n",
    "    post_stft_dataset, nfft, percentage = stft(data_per_window, pruned, verbose)\n",
    "    if verbose: print(\"Data Transformed!\")\n",
    "    if verbose: print(\"===================================================================================\")\n",
    "    if verbose: print(\"\")\n",
    "    if verbose: print(\"Training Data...\")\n",
    "    #print((data_per_window/nfft)*100,\"%\")\n",
    "    X_train, y_train, X_test, y_test = make_train_test(post_stft_dataset, 0.8)\n",
    "    print(X_train.shape, X_test.shape)\n",
    "    train_scr, test_scr = train(100, 1e-8, 'lbfgs', X_train, y_train, X_test, y_test) \n",
    "    return train_scr, test_scr, percentage\n",
    "def run_band(window_len, ratio, verbose):\n",
    "    clfs = OrderedDict()\n",
    "\n",
    "    clfs['Vect + LR'] = make_pipeline(Vectorizer(), StandardScaler(), LogisticRegression())\n",
    "    clfs['Vect + RegLDA'] = make_pipeline(Vectorizer(), LDA(shrinkage='auto', solver='eigen'))\n",
    "    clfs['Xdawn + RegLDA'] = make_pipeline(Xdawn(2, classes=[1]), Vectorizer(), LDA(shrinkage='auto', solver='eigen'))\n",
    "    clfs['ERPCov + TS'] = make_pipeline(ERPCovariances(), TangentSpace(), LogisticRegression())\n",
    "    clfs['ERPCov + MDM'] = make_pipeline(ERPCovariances(), MDM())\n",
    "    print(\"Window size:\",window_len,\"s\")\n",
    "    if verbose: print(\"Loading Data...\")\n",
    "    sample_len, data_per_window, channel = loadData(420, window_len, verbose, [\"1115/binary/7min5.4*.csv\", \"1115/binary/7minrest.csv\"])\n",
    "    if verbose: print(\"===================================================================================\")\n",
    "    if verbose: print(\"\")\n",
    "    if verbose: print(\"Triming Data...\")\n",
    "    pruned = trimData(sample_len, data_per_window, channel, verbose)\n",
    "    if verbose: print(\"===================================================================================\")\n",
    "    if verbose: print(\"\")\n",
    "    if verbose: print(\"Spectralizing Data...\")\n",
    "    spectralized = stcbp(pruned, 256, data_per_window, 0, ratio, 5)\n",
    "    #print(spectralized)\n",
    "    if verbose: print(\"===================================================================================\")\n",
    "    if verbose: print(\"\")\n",
    "    if verbose: print(\"Training Data...\")\n",
    "    #print((data_per_window/nfft)*100,\"%\")\n",
    "    X_train, y_train, X_test, y_test = make_train_test(spectralized, 0.8)\n",
    "    #train_scr, test_scr = train(100, 1e-8, 'lbfgs', X_train, y_train, X_test, y_test)\n",
    "    #return train_scr, test_scr#, percentage\n",
    "    X = np.vstack((X_train,X_test))\n",
    "    X = np.reshape(X, newshape=(X.shape[0],1,X.shape[1]))\n",
    "    y = np.vstack((np.reshape(y_train,newshape=(y_train.shape[0],1)),np.reshape(np.asarray(y_test), newshape=(y_test.shape[0],1))))\n",
    "    y = y.flatten() #crucial, sklearn prefers a vector not an array for labels\n",
    "    assert X.shape[0] == y.shape[0]\n",
    "    # define cross validation \n",
    "    cv = StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\n",
    "\n",
    "    # run cross validation for each pipeline\n",
    "    auc = []\n",
    "    mean = []\n",
    "    methods = []\n",
    "    for m in clfs:\n",
    "        res = cross_val_score(clfs[m], X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "        mean.append(np.mean(res))\n",
    "        auc.extend(res)\n",
    "        methods.extend([m]*len(res))\n",
    "\n",
    "    results = pd.DataFrame(data=auc, columns=['AUC'])\n",
    "    results['Method'] = methods\n",
    "    print(\"The average AUC score is:\",mean)\n",
    "\n",
    "    plt.figure(figsize=[8,4])\n",
    "    sns.barplot(data=results, x='AUC', y='Method')\n",
    "    plt.xlim(0.2, 0.85)\n",
    "    sns.despine()\n",
    "    return mean\n",
    "\n",
    "def run_neurotech(window_len, verbose):\n",
    "    clfs = OrderedDict()\n",
    "\n",
    "    clfs['Vect + LR'] = make_pipeline(Vectorizer(), StandardScaler(), LogisticRegression())\n",
    "    clfs['Vect + RegLDA'] = make_pipeline(Vectorizer(), LDA(shrinkage='auto', solver='eigen'))\n",
    "    clfs['Xdawn + RegLDA'] = make_pipeline(Xdawn(2, classes=[1]), Vectorizer(), LDA(shrinkage='auto', solver='eigen'))\n",
    "    clfs['ERPCov + TS'] = make_pipeline(ERPCovariances(), TangentSpace(), LogisticRegression())\n",
    "    clfs['ERPCov + MDM'] = make_pipeline(ERPCovariances(), MDM())\n",
    "    # format data\n",
    "    print(\"Window size:\",window_len,\"s\")\n",
    "    if verbose: print(\"Loading Data...\")\n",
    "    sample_len, data_per_window, channel = loadData(420, window_len, verbose, [\"1115/binary/7min5.4*.csv\", \"1115/binary/7minrest.csv\"])\n",
    "    if verbose: print(\"===================================================================================\")\n",
    "    if verbose: print(\"\")\n",
    "    if verbose: print(\"Triming Data...\")\n",
    "    pruned = trimData(sample_len, data_per_window, channel, verbose)\n",
    "    if verbose: print(\"===================================================================================\")\n",
    "    if verbose: print(\"\")\n",
    "    if verbose: print(\"STFTing Data...\")\n",
    "    post_stft_dataset, nfft, percentage = stft(data_per_window, pruned, verbose)\n",
    "    if verbose: print(\"Data Transformed!\")\n",
    "    if verbose: print(\"===================================================================================\")\n",
    "    if verbose: print(\"\")\n",
    "    if verbose: print(\"Training Data...\")\n",
    "    \n",
    "    X_train, y_train, X_test, y_test = make_train_test(post_stft_dataset, 0.8)\n",
    "    X = np.vstack((X_train,X_test))\n",
    "    X = np.reshape(X, newshape=(X.shape[0],1,X.shape[1]))\n",
    "    y = np.vstack((np.reshape(y_train,newshape=(y_train.shape[0],1)),np.reshape(np.asarray(y_test), newshape=(y_test.shape[0],1))))\n",
    "    y = y.flatten() #crucial, sklearn prefers a vector not an array for labels\n",
    "    assert X.shape[0] == y.shape[0]\n",
    "    # define cross validation \n",
    "    cv = StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\n",
    "\n",
    "    # run cross validation for each pipeline\n",
    "    auc = []\n",
    "    mean = []\n",
    "    methods = []\n",
    "    for m in clfs:\n",
    "        res = cross_val_score(clfs[m], X, y, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
    "        mean.append(np.mean(res))\n",
    "        auc.extend(res)\n",
    "        methods.extend([m]*len(res))\n",
    "\n",
    "    results = pd.DataFrame(data=auc, columns=['AUC'])\n",
    "    results['Method'] = methods\n",
    "    print(\"The average AUC score is:\",mean)\n",
    "\n",
    "    plt.figure(figsize=[8,4])\n",
    "    sns.barplot(data=results, x='AUC', y='Method')\n",
    "    plt.xlim(0.2, 0.85)\n",
    "    sns.despine()\n",
    "    return mean\n",
    "\n",
    "def validate_with_band(window_len, ratio, verbose):\n",
    "    def calc_accuracy(clf, X_test, y_test):\n",
    "        y_pred = clf.predict(X_test)\n",
    "        cnt = 0\n",
    "        assert y_pred.shape == y_test.shape\n",
    "        for i in range(y_pred.shape[0]):\n",
    "            if y_pred[i] == y_test[i]:\n",
    "                cnt += 1\n",
    "        return cnt/X_test.shape[0]\n",
    "    clfs = OrderedDict()\n",
    "\n",
    "    clfs['Vect + LR'] = make_pipeline(Vectorizer(), StandardScaler(), LogisticRegression())\n",
    "    clfs['Vect + RegLDA'] = make_pipeline(Vectorizer(), LDA(shrinkage='auto', solver='eigen'))\n",
    "    clfs['Xdawn + RegLDA'] = make_pipeline(Xdawn(2, classes=[1]), Vectorizer(), LDA(shrinkage='auto', solver='eigen'))\n",
    "    clfs['ERPCov + TS'] = make_pipeline(ERPCovariances(), TangentSpace(), LogisticRegression())\n",
    "    clfs['ERPCov + MDM'] = make_pipeline(ERPCovariances(), MDM())\n",
    "    print(\"Window size:\",window_len,\"s\")\n",
    "    \n",
    "    X, y = chunky_xy(verbose, window_len, 420, ratio, \"1115/binary/7min5.4*.csv\", \"1115/binary/7minrest.csv\")\n",
    "    X_validate, y_validate = chunky_xy(verbose, window_len, 180, ratio, \"1115/binary/3min5.4*.csv\", \"1115/binary/3minrest.csv\")\n",
    "    cv = StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\n",
    "    # run validation for each pipeline                         \n",
    "    scr = []\n",
    "    methods = []\n",
    "    for m in clfs:\n",
    "        clfs[m].fit(X, y)\n",
    "        #scr.append(calc_accuracy(clfs[m], X_validate, y_validate))\n",
    "        #scr.append(clfs[m].score(X_validate, y_validate))\n",
    "        #methods.extend([m])\n",
    "        \n",
    "        res = cross_val_score(clfs[m], X, y, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
    "        scr.append(clfs[m].score(X_validate, y_validate))\n",
    "        #scr.append(np.mean(res))\n",
    "        methods.extend([m])\n",
    "\n",
    "    results = pd.DataFrame(data=scr, columns=['Accuracy'])\n",
    "    results['Method'] = methods\n",
    "    print(\"The average precision is:\", scr)\n",
    "\n",
    "    plt.figure(figsize=[8,4])\n",
    "    sns.barplot(data=results, x='Accuracy', y='Method')\n",
    "    plt.xlim(0.2, 0.85)\n",
    "    sns.despine()\n",
    "    return scr\n",
    "\n",
    "\n",
    "    \n",
    "win_len_list = [1*i for i in range(1, 2)]\n",
    "ratio_len_list = [0.05*i for i in range(1, 20)]\n",
    "means = []\n",
    "test_res = []\n",
    "train_res = []\n",
    "percent_res = []\n",
    "for w in win_len_list:   \n",
    "    #train_scr, test_scr, percent_scr = run(w, False)\n",
    "    #means.append(run_band(w, 0.4, False))\n",
    "    #means.append(run_neurotech(w, False))\n",
    "    means.append(validate_with_band(w, 0.4, False))\n",
    "    #train_res.append(train_scr)\n",
    "    #test_res.append(test_scr)\n",
    "    #percent_res.append(percent_scr)\n",
    "#for r in ratio_len_list:   \n",
    "    #train_scr, test_scr, percent_scr = run(w, False)\n",
    " #   means.append(run_band(11, r, False))\n",
    "    #means.append(run_neurotech(w, False))\n",
    "    #train_res.append(train_scr)\n",
    "    #test_res.append(test_scr)\n",
    "    #percent_res.append(percent_scr)   \n",
    "means = np.array(means)\n",
    "\n",
    "\n",
    "#plt.plot(win_len_list, train_res, label = \"Train Score\")\n",
    "#plt.plot(win_len_list, test_res, label = \"Test Score\")\n",
    "#plt.plot(win_len_list, percent_res, label = \"Percentage Score\")\n",
    "#plt.xlabel(\"Window Length (s)\")\n",
    "#plt.ylabel(\"Score\")\n",
    "#plt.title(\"Score VS Window Length\")\n",
    "#plt.legend()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_means = [max(means[:,i]) for i in range(len(means[0]))]\n",
    "print(max_means)\n",
    "mean_means = [np.mean(means[:,i]) for i in range(len(means[0]))]\n",
    "print(mean_means)\n",
    "#print(np.mean(test_res))\n",
    "#print(np.mean(train_res))\n",
    "#SGD Mean: Test 0.4848184223184223, Train 0.5188464252400026\n",
    "#LBFGS 1MIN Mean: Test 0.6651543651414771, Train 0.9402328392207571\n",
    "#LBFGS 1MIN Mean Standarized: Test 0.4944845437674585, Train 1.0 seems like normalization makes it worse\n",
    "#LBFGS 10MIN Mean: Test: 0.6221004202855277, Train 0.7853136992342494\n",
    "#ADAM Mean: Test 0.5002369225627413, Train 0.9166582700882311\n",
    "\n",
    "plt.plot(win_len_list, means[:,0], label = 'Vect + LR')\n",
    "plt.plot(win_len_list, means[:,1], label = 'Vect + RegLDA')\n",
    "plt.plot(win_len_list, means[:,2], label = 'Xdawn + RegLDA')\n",
    "plt.plot(win_len_list, means[:,3], label = 'ERPCov + TS')\n",
    "plt.plot(win_len_list, means[:,4], label = 'ERPCov + MDM')\n",
    "\n",
    "plt.xlabel(\"Window Length (s)\")\n",
    "plt.ylabel(\"AUC Score\")\n",
    "plt.title(\"Score VS Window Length\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ugly Code Ahead. DO NOT TRESPASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca = conglomerates(post_stft_dataset)\n",
    "print(X_pca.shape)\n",
    "\n",
    "#try neural network on PCAed data\n",
    "pc_num = 10\n",
    "\n",
    "print(\"Doing PCA analysis ...\")\n",
    "start_time = time.time()\n",
    "pca = PCA(n_components = pc_num)\n",
    "pca.fit(X_pca)\n",
    "mean = pca.mean_\n",
    "pc = pca.components_[:pc_num]\n",
    "end_time = time.time()\n",
    "print(\"Done!\")\n",
    "print(\"Duration: \", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA_project(data, new_basis, mean):\n",
    "    #To see the data's repr in the basis of principal components\n",
    "    return np.dot((data-mean), new_basis.T)\n",
    "def deconglomerates(post_pca_two_d_matrix):\n",
    "    three_d = []\n",
    "    divider = (int)(post_pca_two_d_matrix.shape[0]/post_stft_dataset.shape[0])\n",
    "    assert post_pca_two_d_matrix.shape[0]%post_stft_dataset.shape[0] == 0\n",
    "    three_d = [post_pca_two_d_matrix[i*divider:(i+1)*divider] for i in range(0,post_stft_dataset.shape[0])]\n",
    "    return np.asarray(three_d)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16,9))\n",
    "proj = PCA_project(X_pca, pc, mean)\n",
    "print(proj.shape)\n",
    "PCA_projected_dataset = deconglomerates(proj)\n",
    "print(PCA_projected_dataset.shape)\n",
    "assert PCA_projected_dataset.shape[0] == post_stft_dataset.shape[0]\n",
    "assert PCA_projected_dataset.shape[1] == post_stft_dataset.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xp_train, Xp_test, yp_train, yp_test = [],[],[],[]\n",
    "for i in range(PCA_projected_dataset.shape[0]):\n",
    "    xtr, xt, ytr, yt = three_d_train_test_split(PCA_projected_dataset[i],i,0.8)\n",
    "    Xp_train.append(xtr)\n",
    "    Xp_test.append(xt)\n",
    "    yp_train.append(ytr)\n",
    "    yp_test.append(yt)\n",
    "Xp_train = conglomerates(np.asarray(Xp_train))\n",
    "Xp_test = conglomerates(np.asarray(Xp_test))\n",
    "yp_train = np.asarray(yp_train).flatten()\n",
    "yp_test = np.asarray(yp_test).flatten()\n",
    "\n",
    "assert Xp_train.shape[0] == yp_train.shape[0]\n",
    "assert Xp_test.shape[0] == yp_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(200,20), max_iter=50, alpha=1e-6,\n",
    "                    solver='adam', verbose=10, tol=1e-4, random_state=1,\n",
    "                    learning_rate_init=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = inf\n",
      "Iteration 2, loss = 1.23988586\n",
      "Iteration 3, loss = 1.10148277\n",
      "Iteration 4, loss = 1.10038323\n",
      "Iteration 5, loss = 1.10293012\n",
      "Iteration 6, loss = 1.10282839\n",
      "Iteration 7, loss = 1.10049364\n",
      "Iteration 8, loss = 1.10022342\n",
      "Iteration 9, loss = 1.09981903\n",
      "Iteration 10, loss = 1.10235450\n",
      "Iteration 11, loss = 1.09932167\n",
      "Iteration 12, loss = 1.09948772\n",
      "Iteration 13, loss = 1.10111882\n",
      "Iteration 14, loss = 1.10548646\n",
      "Iteration 15, loss = 1.10011708\n",
      "Iteration 16, loss = 1.10012716\n",
      "Iteration 17, loss = 1.10559643\n",
      "Iteration 18, loss = 1.10305810\n",
      "Iteration 19, loss = 1.10029459\n",
      "Iteration 20, loss = 1.10415273\n",
      "Iteration 21, loss = 1.10057255\n",
      "Iteration 22, loss = 1.10705179\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training set score: 0.333333\n",
      "Test set score: 0.333333\n"
     ]
    }
   ],
   "source": [
    "mlp.fit(Xp_train, yp_train)\n",
    "print(\"Training set score: %f\" % mlp.score(Xp_train, yp_train))\n",
    "print(\"Test set score: %f\" % mlp.score(Xp_test, yp_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "(array([], dtype=int64),)\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1,2,3])\n",
    "b, = np.where(a > 3)\n",
    "print(b)\n",
    "b = np.where(a > 3)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
