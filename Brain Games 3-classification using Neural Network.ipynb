{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brain Games 3-classification using Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check neurotech https://github.com/neurotech-berkeley\n",
    "#check pyriemman https://github.com/alexandrebarachant\n",
    "#definitely break down to waves and train them (P300, Alpha...)-->get relative power\n",
    "#Plot power spetrum to see if there is noise in the higher frequencies\n",
    "#Maybe Lasso Regression when we are getting zero-padded sparse fft data\n",
    "#Focus != Difficulty\n",
    "#Detect Relaxation (grow flowers, encourage relaxation)\n",
    "#Real time stream automatic data preprocessing problem\n",
    "#K-mean clustering fuzzy coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import sklearn\n",
    "import scipy.cluster\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy import signal\n",
    "import time\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.utils import shuffle\n",
    "import math\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import mne\n",
    "from mne.decoding import Vectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.model_selection import cross_val_score, StratifiedShuffleSplit\n",
    "#Try directly installing pyriemann in jupyter notebook if conda/pip doesn't make it\n",
    "from pyriemann.estimation import ERPCovariances\n",
    "from pyriemann.tangentspace import TangentSpace\n",
    "from pyriemann.classification import MDM\n",
    "from pyriemann.spatialfilters import Xdawn\n",
    "from collections import OrderedDict\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(sample_time, window_time, verbose, *args):\n",
    "    csvData = []\n",
    "    for a in args:\n",
    "        csvData.append(np.genfromtxt(a, delimiter=','))\n",
    "    min_len = min([c.shape[0] for c in csvData])\n",
    "    if verbose: print(\"There are {l} raw data points\".format(l=min_len))\n",
    "    csvData = [c[:min_len,:] for c in csvData]\n",
    "    sample_len = csvData[0].shape[0] - 1\n",
    "    window_num = (int)(sample_time//window_time) \n",
    "    data_per_window = (int)(sample_len//(window_num)) #data_per_window = datapoints per window = sample_len // (window_num)\n",
    "    #assert(data_per_window % 2 == 0)\n",
    "\n",
    "    if verbose: print(\"There are\",data_per_window, \"datapoints per window, and there are\", window_num, \"independent windows.\")\n",
    "    channel = []\n",
    "    for d in csvData:\n",
    "        channel.append(d[1:,1:])\n",
    "        \"\"\"print(d.shape)\n",
    "        plt.plot(d[2000:-1000,2:3])\n",
    "        plt.xlim((0,d.shape[0]))\n",
    "        plt.title('A Very Representative Feature VS Full Time')\n",
    "        plt.figure(figsize = (16,9))\"\"\"\n",
    "    #print(np.asarray(channel).shape)\n",
    "    #trimData(sample_len, data_per_window, channel)\n",
    "    return sample_len, data_per_window, channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trimData(sample_len, data_per_window, channel, verbose):\n",
    "    residual = sample_len % data_per_window\n",
    "    if verbose: print(\"There are\", residual, \"points left out by segmentation, now cutting them from both ends...\")\n",
    "    residualStart = residual // 2\n",
    "    residualEnd = residual - residualStart\n",
    "    residualEnd = len(channel[0]) - residualEnd\n",
    "    #print(len(channel[0]), residualStart, residualEnd)\n",
    "\n",
    "    pruned = []\n",
    "    for c in channel:\n",
    "        pruned.append(c[residualStart : residualEnd,:])\n",
    "    pruned = np.asarray(pruned)\n",
    "    if verbose: print(\"The shape of the pruned dataset is: \", pruned.shape)\n",
    "    if verbose: print(\"The first number stands for the number of classes, the second number stands for raw data points and the third number stands for the number of channels\")\n",
    "    for p in pruned:\n",
    "        assert np.asarray(p).shape[0] % data_per_window == 0, np.asarray(p).shape[0] % data_per_window\n",
    "    return pruned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_fft_len(data_per_window):\n",
    "    result = 2**math.ceil(math.log2(data_per_window))\n",
    "    return result, data_per_window/result\n",
    "\n",
    "def stft_for_one_channel(nfft, data_per_window, data):\n",
    "    #nfft should be the closest power of 2 to data_per_window\n",
    "    f,t,Zxx = scipy.signal.stft(data, nperseg=data_per_window, return_onesided=False, padded=True, nfft=nfft)\n",
    "    #assert (Zxx.shape[1] == 2*window_num*len(csvData) + 1)\n",
    "    #print(\"There are\", Zxx.shape[1],\"time window captured by STFT...\")\n",
    "    Zxx = Zxx.T #Zxx = new data matrix --> time windows are our new data points and frequency are the features\n",
    "    #We sacrificed segements of data to expand feature size from 1 to 256\n",
    "    #print(Zxx.shape)\n",
    "    #print(\"Parsing each DFT to extract the actual coefficients used in the trucated Fourier Series...\")\n",
    "    for i in range(0,Zxx.shape[0]):\n",
    "        chnl = Zxx[i]\n",
    "        chnl[0] = chnl[0].real\n",
    "        chnl[1:nfft//2] = 1*chnl[1:nfft//2].real\n",
    "        chnl[nfft//2] = chnl[nfft//2]\n",
    "        chnl[nfft//2 + 1:] = (1)*chnl[nfft//2 + 1:].imag\n",
    "    assert ((Zxx.real == Zxx).all())\n",
    "    #print(\"Coefficient extracted!\")\n",
    "    Zxx = Zxx.real #The imaginary compoenent is 0 by now, we are just getting rid of the j for PCA\n",
    "    #Since PCA from sklearn doesn't handle complex data, let us simply use the weight of thecosine part of the frequency space\n",
    "    return Zxx\n",
    "\n",
    "\n",
    "def stft(data_per_window, pruned, verbose):\n",
    "    nfft, percentage = find_fft_len(data_per_window) #depending on the window size\n",
    "    if verbose: print(\"FFT length: \", nfft)\n",
    "    #We again reduce the data size and increase the features from 5 to 5*nfft\n",
    "    post_stft_dataset = []\n",
    "    if verbose: print(\"Parsing each DFT to extract the actual coefficients used in the trucated Fourier Series...\")\n",
    "    for p in pruned:\n",
    "        expanded_features_dataset = []\n",
    "        for i in range(p.shape[1]):\n",
    "            channel = p[:,i]\n",
    "            stft_results = stft_for_one_channel(nfft, data_per_window, channel)\n",
    "            expanded_features_dataset.append(stft_results)\n",
    "        expanded_features_dataset = np.asarray(expanded_features_dataset)\n",
    "        #We will hstack the the stfted results for each channel for this particular class\n",
    "        hstacked = expanded_features_dataset[0]\n",
    "        for expanded_channel in expanded_features_dataset[1:]:\n",
    "            hstacked = np.hstack((hstacked, expanded_channel))\n",
    "        post_stft_dataset.append(hstacked)\n",
    "    if verbose: print(\"Coefficient extracted!\")\n",
    "    post_stft_dataset = np.asarray(post_stft_dataset)\n",
    "    if verbose: print(\"The transformed dataset after STFT: \", post_stft_dataset.shape)\n",
    "    if verbose: print(\"The first number stands for the number of classes, the second number stands for new data points and the third number stands for the number of features\")\n",
    "    return post_stft_dataset, nfft, percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def three_d_train_test_split(three_d_dataset, label, ratio):\n",
    "    #Outputs 3d matrices for train and test set\n",
    "    split = (int)(three_d_dataset.shape[0] * ratio)\n",
    "    y = label * np.ones(three_d_dataset.shape[0])\n",
    "    return three_d_dataset[:split], three_d_dataset[split:], y[:split], y[split:]\n",
    "\n",
    "def conglomerates(three_d_matrix):\n",
    "    #Outputs 2d matrices, stacked along the first axis\n",
    "    vstacked = three_d_matrix[0]\n",
    "    for m in three_d_matrix[1:]:\n",
    "        vstacked = np.vstack((vstacked, m))\n",
    "    return vstacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_test(post_stft_dataset, ratio):\n",
    "    X_train, X_test, y_train, y_test = [],[],[],[]\n",
    "    for i in range(post_stft_dataset.shape[0]):\n",
    "        xtr, xt, ytr, yt = three_d_train_test_split(post_stft_dataset[i],i,0.8)\n",
    "        X_train.append(xtr)\n",
    "        X_test.append(xt)\n",
    "        y_train.append(ytr)\n",
    "        y_test.append(yt)\n",
    "    X_train = conglomerates(np.asarray(X_train))\n",
    "    X_test = conglomerates(np.asarray(X_test))\n",
    "    y_train = np.asarray(y_train).flatten()\n",
    "    y_test = np.asarray(y_test).flatten()\n",
    "    \n",
    "    assert X_train.shape[0] == y_train.shape[0]\n",
    "    assert X_test.shape[0] == y_test.shape[0]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(max_iter, alpha, solver, X_train, y_train, X_test, y_test):\n",
    "    #adam, lbfgs, sgd\n",
    "    #alhpa: 1e-8\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(200,20), max_iter=max_iter, alpha=alpha,\n",
    "                    solver=solver, verbose=10, tol=1e-4, random_state=1,\n",
    "                    learning_rate_init=.1)\n",
    "    mlp.fit(X_train, y_train)\n",
    "    train_scr = mlp.score(X_train, y_train)\n",
    "    test_scr = mlp.score(X_test, y_test)\n",
    "    print(\"Training set score: %f\" % train_scr)\n",
    "    print(\"Test set score: %f\" % test_scr)\n",
    "    return train_scr, test_scr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "The average AUC score is: [0.6197278911564625, 0.5650085034013606, 0.5650085034013606, 0.7917091836734693, 0.6893707482993197]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def run(window_len, verbose):\n",
    "    print(window_len)\n",
    "    if verbose: print(\"Loading Data...\")\n",
    "    sample_len, data_per_window, channel = loadData(60, window_len, verbose, \"60sactualrest.csv\", \"60smultiplication5.4lvl.csv\")\n",
    "    if verbose: print(\"===================================================================================\")\n",
    "    if verbose: print(\"\")\n",
    "    if verbose: print(\"Triming Data...\")\n",
    "    pruned = trimData(sample_len, data_per_window, channel, verbose)\n",
    "    if verbose: print(\"===================================================================================\")\n",
    "    if verbose: print(\"\")\n",
    "    if verbose: print(\"STFTing Data...\")\n",
    "    post_stft_dataset, nfft, percentage = stft(data_per_window, pruned, verbose)\n",
    "    if verbose: print(\"Data Transformed!\")\n",
    "    if verbose: print(\"===================================================================================\")\n",
    "    if verbose: print(\"\")\n",
    "    if verbose: print(\"Training Data...\")\n",
    "    #print((data_per_window/nfft)*100,\"%\")\n",
    "    X_train, y_train, X_test, y_test = make_train_test(post_stft_dataset, 0.8)\n",
    "    train_scr, test_scr = train(100, 1e-8, 'lbfgs', X_train, y_train, X_test, y_test)\n",
    "    return train_scr, test_scr, percentage\n",
    "def run_neurotech(window_len, verbose):\n",
    "    clfs = OrderedDict()\n",
    "\n",
    "    clfs['Vect + LR'] = make_pipeline(Vectorizer(), StandardScaler(), LogisticRegression())\n",
    "    clfs['Vect + RegLDA'] = make_pipeline(Vectorizer(), LDA(shrinkage='auto', solver='eigen'))\n",
    "    clfs['Xdawn + RegLDA'] = make_pipeline(Xdawn(2, classes=[1]), Vectorizer(), LDA(shrinkage='auto', solver='eigen'))\n",
    "    clfs['ERPCov + TS'] = make_pipeline(ERPCovariances(), TangentSpace(), LogisticRegression())\n",
    "    clfs['ERPCov + MDM'] = make_pipeline(ERPCovariances(), MDM())\n",
    "    # format data\n",
    "    print(window_len)\n",
    "    if verbose: print(\"Loading Data...\")\n",
    "    sample_len, data_per_window, channel = loadData(60, window_len, verbose, \"60sactualrest.csv\", \"60smultiplication5.4lvl.csv\")\n",
    "    if verbose: print(\"===================================================================================\")\n",
    "    if verbose: print(\"\")\n",
    "    if verbose: print(\"Triming Data...\")\n",
    "    pruned = trimData(sample_len, data_per_window, channel, verbose)\n",
    "    if verbose: print(\"===================================================================================\")\n",
    "    if verbose: print(\"\")\n",
    "    if verbose: print(\"STFTing Data...\")\n",
    "    post_stft_dataset, nfft, percentage = stft(data_per_window, pruned, verbose)\n",
    "    if verbose: print(\"Data Transformed!\")\n",
    "    if verbose: print(\"===================================================================================\")\n",
    "    if verbose: print(\"\")\n",
    "    if verbose: print(\"Training Data...\")\n",
    "    \n",
    "    X_train, y_train, X_test, y_test = make_train_test(post_stft_dataset, 0.8)\n",
    "    X = np.vstack((X_train,X_test))\n",
    "    X = np.reshape(X, newshape=(X.shape[0],1,X.shape[1]))\n",
    "    y = np.vstack((np.reshape(y_train,newshape=(y_train.shape[0],1)),np.reshape(np.asarray(y_test), newshape=(y_test.shape[0],1))))\n",
    "    y = y.flatten() #crucial, sklearn prefers a vector not an array for labels\n",
    "    assert X.shape[0] == y.shape[0]\n",
    "    # define cross validation \n",
    "    cv = StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\n",
    "\n",
    "    # run cross validation for each pipeline\n",
    "    auc = []\n",
    "    mean = []\n",
    "    methods = []\n",
    "    for m in clfs:\n",
    "        res = cross_val_score(clfs[m], X, y, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
    "        mean.append(np.mean(res))\n",
    "        auc.extend(res)\n",
    "        methods.extend([m]*len(res))\n",
    "\n",
    "    results = pd.DataFrame(data=auc, columns=['AUC'])\n",
    "    results['Method'] = methods\n",
    "    print(\"The average AUC score is:\",mean)\n",
    "\n",
    "    plt.figure(figsize=[8,4])\n",
    "    sns.barplot(data=results, x='AUC', y='Method')\n",
    "    plt.xlim(0.2, 0.85)\n",
    "    sns.despine()\n",
    "\n",
    "    \n",
    "win_len_list = [0.5*i for i in range(1, 2)]\n",
    "test_res = []\n",
    "train_res = []\n",
    "percent_res = []\n",
    "for w in win_len_list:   \n",
    "    #train_scr, test_scr, percent_scr = run_neurotech(w, False)\n",
    "    run_neurotech(w, False)\n",
    "    #train_res.append(train_scr)\n",
    "    #test_res.append(test_scr)\n",
    "    #percent_res.append(percent_scr)\n",
    "\n",
    "#plt.plot(win_len_list, train_res, label = \"Train Score\")\n",
    "#plt.plot(win_len_list, test_res, label = \"Test Score\")\n",
    "#plt.plot(win_len_list, percent_res, label = \"Percentage Score\")\n",
    "#plt.xlabel(\"Window Length (s)\")\n",
    "#plt.ylabel(\"Score\")\n",
    "#plt.title(\"Score VS Window Length\")\n",
    "#plt.legend()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.652958998779444\n",
      "0.9353047934479269\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(test_res))\n",
    "print(np.mean(train_res))\n",
    "#SGD Mean: Test 0.4848184223184223, Train 0.5188464252400026\n",
    "#LBFGS 1MIN Mean: Test 0.6651543651414771, Train 0.9402328392207571\n",
    "#LBFGS 1MIN Mean Standarized: Test 0.4944845437674585, Train 1.0 seems like normalization makes it worse\n",
    "#LBFGS 10MIN Mean: Test: 0.6221004202855277, Train 0.7853136992342494\n",
    "#ADAM Mean: Test 0.5002369225627413, Train 0.9166582700882311"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca = conglomerates(post_stft_dataset)\n",
    "print(X_pca.shape)\n",
    "\n",
    "#try neural network on PCAed data\n",
    "pc_num = 10\n",
    "\n",
    "print(\"Doing PCA analysis ...\")\n",
    "start_time = time.time()\n",
    "pca = PCA(n_components = pc_num)\n",
    "pca.fit(X_pca)\n",
    "mean = pca.mean_\n",
    "pc = pca.components_[:pc_num]\n",
    "end_time = time.time()\n",
    "print(\"Done!\")\n",
    "print(\"Duration: \", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA_project(data, new_basis, mean):\n",
    "    #To see the data's repr in the basis of principal components\n",
    "    return np.dot((data-mean), new_basis.T)\n",
    "def deconglomerates(post_pca_two_d_matrix):\n",
    "    three_d = []\n",
    "    divider = (int)(post_pca_two_d_matrix.shape[0]/post_stft_dataset.shape[0])\n",
    "    assert post_pca_two_d_matrix.shape[0]%post_stft_dataset.shape[0] == 0\n",
    "    three_d = [post_pca_two_d_matrix[i*divider:(i+1)*divider] for i in range(0,post_stft_dataset.shape[0])]\n",
    "    return np.asarray(three_d)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16,9))\n",
    "proj = PCA_project(X_pca, pc, mean)\n",
    "print(proj.shape)\n",
    "PCA_projected_dataset = deconglomerates(proj)\n",
    "print(PCA_projected_dataset.shape)\n",
    "assert PCA_projected_dataset.shape[0] == post_stft_dataset.shape[0]\n",
    "assert PCA_projected_dataset.shape[1] == post_stft_dataset.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xp_train, Xp_test, yp_train, yp_test = [],[],[],[]\n",
    "for i in range(PCA_projected_dataset.shape[0]):\n",
    "    xtr, xt, ytr, yt = three_d_train_test_split(PCA_projected_dataset[i],i,0.8)\n",
    "    Xp_train.append(xtr)\n",
    "    Xp_test.append(xt)\n",
    "    yp_train.append(ytr)\n",
    "    yp_test.append(yt)\n",
    "Xp_train = conglomerates(np.asarray(Xp_train))\n",
    "Xp_test = conglomerates(np.asarray(Xp_test))\n",
    "yp_train = np.asarray(yp_train).flatten()\n",
    "yp_test = np.asarray(yp_test).flatten()\n",
    "\n",
    "assert Xp_train.shape[0] == yp_train.shape[0]\n",
    "assert Xp_test.shape[0] == yp_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(200,20), max_iter=50, alpha=1e-6,\n",
    "                    solver='adam', verbose=10, tol=1e-4, random_state=1,\n",
    "                    learning_rate_init=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = inf\n",
      "Iteration 2, loss = 1.23988586\n",
      "Iteration 3, loss = 1.10148277\n",
      "Iteration 4, loss = 1.10038323\n",
      "Iteration 5, loss = 1.10293012\n",
      "Iteration 6, loss = 1.10282839\n",
      "Iteration 7, loss = 1.10049364\n",
      "Iteration 8, loss = 1.10022342\n",
      "Iteration 9, loss = 1.09981903\n",
      "Iteration 10, loss = 1.10235450\n",
      "Iteration 11, loss = 1.09932167\n",
      "Iteration 12, loss = 1.09948772\n",
      "Iteration 13, loss = 1.10111882\n",
      "Iteration 14, loss = 1.10548646\n",
      "Iteration 15, loss = 1.10011708\n",
      "Iteration 16, loss = 1.10012716\n",
      "Iteration 17, loss = 1.10559643\n",
      "Iteration 18, loss = 1.10305810\n",
      "Iteration 19, loss = 1.10029459\n",
      "Iteration 20, loss = 1.10415273\n",
      "Iteration 21, loss = 1.10057255\n",
      "Iteration 22, loss = 1.10705179\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training set score: 0.333333\n",
      "Test set score: 0.333333\n"
     ]
    }
   ],
   "source": [
    "mlp.fit(Xp_train, yp_train)\n",
    "print(\"Training set score: %f\" % mlp.score(Xp_train, yp_train))\n",
    "print(\"Test set score: %f\" % mlp.score(Xp_test, yp_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
